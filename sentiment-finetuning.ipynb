{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-12-31T06:54:13.903068Z","iopub.status.busy":"2024-12-31T06:54:13.902779Z","iopub.status.idle":"2024-12-31T06:54:20.158684Z","shell.execute_reply":"2024-12-31T06:54:20.157827Z","shell.execute_reply.started":"2024-12-31T06:54:13.903037Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting evaluate\n","  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n","Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.1.4)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.5)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n","Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.24.7)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.10.5)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.11.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n","Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: evaluate\n","Successfully installed evaluate-0.4.3\n"]}],"source":["!pip install  evaluate"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-12-31T06:54:20.194069Z","iopub.status.busy":"2024-12-31T06:54:20.193819Z","iopub.status.idle":"2024-12-31T06:54:46.924457Z","shell.execute_reply":"2024-12-31T06:54:46.923574Z","shell.execute_reply.started":"2024-12-31T06:54:20.194050Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","from transformers import AutoModelForSeq2SeqLM, AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments,AutoModelForSequenceClassification\n","from datasets import load_dataset\n","from sklearn.model_selection import train_test_split\n","import torch\n","import evaluate\n","from transformers import DataCollatorWithPadding"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-12-31T06:54:48.899568Z","iopub.status.busy":"2024-12-31T06:54:48.898961Z","iopub.status.idle":"2024-12-31T06:54:49.049808Z","shell.execute_reply":"2024-12-31T06:54:49.049115Z","shell.execute_reply.started":"2024-12-31T06:54:48.899538Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: fineGrained).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["from huggingface_hub import login\n","\n","# Set the token as an environment variable\n","hf_token = \"[my_hf_token]\"\n","\n","login(hf_token)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-12-31T06:54:50.732107Z","iopub.status.busy":"2024-12-31T06:54:50.731797Z","iopub.status.idle":"2024-12-31T06:54:53.036016Z","shell.execute_reply":"2024-12-31T06:54:53.035238Z","shell.execute_reply.started":"2024-12-31T06:54:50.732081Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"36de635eaf3f404485d6a48ac406dd13","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"45757b0791b54cff9e39271c0552574e","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e5ddb6c4ee0847f6b73fdce94b1547de","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9db89e12659c4ee8b80fc3c25874265b","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7675d8e72f304d338af1d18f0a052086","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}],"source":["# Import Model\n","model_name = 'distilbert/distilbert-base-uncased'\n","model = AutoModelForSequenceClassification.from_pretrained(model_name)\n","\n","# Import Tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-12-31T06:54:57.489922Z","iopub.status.busy":"2024-12-31T06:54:57.489612Z","iopub.status.idle":"2024-12-31T06:54:57.493423Z","shell.execute_reply":"2024-12-31T06:54:57.492713Z","shell.execute_reply.started":"2024-12-31T06:54:57.489896Z"},"trusted":true},"outputs":[],"source":["# Define label maps\n","id2label = {0: \"Negative\", 1: \"Positive\"}\n","label2id = {\"Negative\":0, \"Positive\":1}"]},{"cell_type":"markdown","metadata":{},"source":["# Data Pre-processing"]},{"cell_type":"markdown","metadata":{},"source":["## Import Data & Train-test-split"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-12-31T06:54:59.178966Z","iopub.status.busy":"2024-12-31T06:54:59.178545Z","iopub.status.idle":"2024-12-31T06:55:01.997863Z","shell.execute_reply":"2024-12-31T06:55:01.997167Z","shell.execute_reply.started":"2024-12-31T06:54:59.178933Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6bf27852c44542a3b2a7a1dd57db9bf0","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c155e2f0a18a48bf8bf2b5e5e94ac6e5","version_major":2,"version_minor":0},"text/plain":["train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ea7f1a12816d4a93b7a78369ccb0401c","version_major":2,"version_minor":0},"text/plain":["test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f310222c754d4986bcbc340a2b7ab105","version_major":2,"version_minor":0},"text/plain":["unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"59fbffe4f2994fd2b5ac701fe1f14e28","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"31eb7529db18441a8ab09d2e8f2f9aa5","version_major":2,"version_minor":0},"text/plain":["Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7a42a91854ba420e8e9c7eba711db37d","version_major":2,"version_minor":0},"text/plain":["Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["------Before Splitting------\n","(25000, 2)\n","(25000, 2)\n","(50000, 2)\n","------After Splitting------\n","Train Data: (25000, 2)\n","Validation Data: (5000, 2)\n","Test Data: (20000, 2)\n","Unsupervised Data: (50000, 2)\n"]}],"source":["# Import Training Dataset\n","ds = load_dataset('stanfordnlp/imdb')\n","\n","print('------Before Splitting------')\n","print(ds['train'].shape)\n","print(ds['test'].shape)\n","print(ds['unsupervised'].shape)\n","\n","train_data = ds['train']\n","test_data_raw = ds['test']\n","unsupervised_data = ds['unsupervised']\n","\n","split_ds = test_data_raw.train_test_split(test_size=0.8, stratify_by_column='label', seed=42)\n","\n","# Extract the validation and test datasets\n","val_data = split_ds['train']\n","test_data = split_ds['test']\n","\n","\n","print('------After Splitting------')\n","print(f'Train Data: {train_data.shape}')\n","print(f'Validation Data: {val_data.shape}')\n","print(f'Test Data: {test_data.shape}')\n","print(f'Unsupervised Data: {unsupervised_data.shape}')"]},{"cell_type":"markdown","metadata":{},"source":["## Tokenize"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-12-31T06:55:04.625724Z","iopub.status.busy":"2024-12-31T06:55:04.625408Z","iopub.status.idle":"2024-12-31T06:55:33.478965Z","shell.execute_reply":"2024-12-31T06:55:33.478243Z","shell.execute_reply.started":"2024-12-31T06:55:04.625702Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f5cb6ee7f53b45f78dabf06617fd1ac1","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ffde6eede9c34240aad4111d1eb20f58","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ec32fe1a336548b3997441566076e7d0","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/20000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["['label', 'input_ids', 'attention_mask']\n","['label', 'input_ids', 'attention_mask']\n","['label', 'input_ids', 'attention_mask']\n"]}],"source":["def tokenize_function(examples):\n","    \"\"\"\n","    Tokenizes the input text and moves tensors to the GPU.\n","\n","    Args:\n","        examples: A dictionary containing the input text ('text').\n","\n","    Returns:\n","        A dictionary containing the tokenized inputs (e.g., 'input_ids', 'attention_mask') \n","        on the GPU.\n","    \"\"\"\n","    result = tokenizer(examples['text'], \n","                       padding='max_length', \n","                       truncation=True, \n","                       return_tensors='pt') \n","        \n","    return result\n","\n","# Apply the tokenizer to the datasets\n","train_data_tokenized = train_data.map(tokenize_function, batched=True, load_from_cache_file=False)\n","val_data_tokenized = val_data.map(tokenize_function, batched=True, load_from_cache_file=False)\n","test_data_tokenized = test_data.map(tokenize_function, batched=True, load_from_cache_file=False)\n","\n","# Drop the 'text' column from the datasets\n","train_data_tokenized = train_data_tokenized.remove_columns(['text'])\n","val_data_tokenized = val_data_tokenized.remove_columns(['text'])\n","test_data_tokenized = test_data_tokenized.remove_columns(['text'])\n","\n","# Initialize the data collator\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","print(train_data_tokenized.column_names)\n","print(val_data_tokenized.column_names)\n","print(test_data_tokenized.column_names)"]},{"cell_type":"markdown","metadata":{},"source":["# Estimate performance before fine-tuning"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-12-31T07:46:59.829732Z","iopub.status.busy":"2024-12-31T07:46:59.829432Z","iopub.status.idle":"2024-12-31T07:46:59.987697Z","shell.execute_reply":"2024-12-31T07:46:59.986806Z","shell.execute_reply.started":"2024-12-31T07:46:59.829709Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Untrained model predictions: \n","\n","If only to avoid making this type of film in the future. : Positive\n","It was amazing. : Positive\n","I want my money back : Positive\n","such a waste of time : Negative\n","Greatest show ever : Positive\n"]}],"source":["# define list of examples\n","text_list = [\"If only to avoid making this type of film in the future.\",\"It was amazing.\", \"I want my money back\",\"such a waste of time\",\"Greatest show ever\"]\n","\n","print(\"Untrained model predictions: \\n\" )\n","\n","for text in text_list:\n","    inputs = tokenizer.encode(text, return_tensors=\"pt\")\n","    # compute logits\n","    logits = model(inputs).logits\n","    # convert logits to label\n","    predictions = torch.argmax(logits, dim=-1).item()\n","\n","    print(text + \" : \" + id2label[predictions])"]},{"cell_type":"markdown","metadata":{},"source":["# Fine Tuning Starts"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-12-31T06:40:28.322712Z","iopub.status.busy":"2024-12-31T06:40:28.322317Z","iopub.status.idle":"2024-12-31T06:40:28.329930Z","shell.execute_reply":"2024-12-31T06:40:28.328589Z","shell.execute_reply.started":"2024-12-31T06:40:28.322679Z"},"trusted":true},"outputs":[{"data":{"text/plain":["DistilBertForSequenceClassification(\n","  (distilbert): DistilBertModel(\n","    (embeddings): Embeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (transformer): Transformer(\n","      (layer): ModuleList(\n","        (0-5): 6 x TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","            (activation): GELUActivation()\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","      )\n","    )\n","  )\n","  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.2, inplace=False)\n",")"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# Check model architecture\n","model"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-12-31T06:55:38.020779Z","iopub.status.busy":"2024-12-31T06:55:38.020488Z","iopub.status.idle":"2024-12-31T06:55:38.025042Z","shell.execute_reply":"2024-12-31T06:55:38.024335Z","shell.execute_reply.started":"2024-12-31T06:55:38.020757Z"},"trusted":true},"outputs":[],"source":["# Freeze base layers parameters, leave Classification Heads unfrozen\n","for param in model.distilbert.parameters():\n","    param.requires_grad = False\n","\n","\n","# Unfreeze the classification head (last layer)\n","for param in model.classifier.parameters():\n","    param.requires_grad = True"]},{"cell_type":"markdown","metadata":{},"source":["### Defining computing metric"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-12-31T06:55:39.617532Z","iopub.status.busy":"2024-12-31T06:55:39.617192Z","iopub.status.idle":"2024-12-31T06:55:40.014037Z","shell.execute_reply":"2024-12-31T06:55:40.013440Z","shell.execute_reply.started":"2024-12-31T06:55:39.617504Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6d63868e5e944ad6b50e9a20e130da29","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Load the accuracy metric\n","accuracy = evaluate.load(\"accuracy\")\n","\n","def compute_metrics(p):\n","    # p is an instance of EvalPrediction\n","    predictions = np.argmax(p.predictions, axis=1)  # Get the predicted class indices\n","    labels = p.label_ids  # True labels\n","    \n","    # Compute accuracy\n","    return {\"accuracy\": accuracy.compute(predictions=predictions, references=labels)}\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-12-31T06:56:37.726138Z","iopub.status.busy":"2024-12-31T06:56:37.725841Z","iopub.status.idle":"2024-12-31T06:56:37.878586Z","shell.execute_reply":"2024-12-31T06:56:37.877884Z","shell.execute_reply.started":"2024-12-31T06:56:37.726118Z"},"trusted":true},"outputs":[],"source":["# hyperparameters\n","lr = 2e-4\n","batch_size = 20 # Check GPU Memory to adjust, when batch_size=4, 2 T4 GPUs have ~ 1GB memory out of 15GB.\n","num_epochs = 8\n","\n","# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"/kaggle/working/results\",  \n","    learning_rate=lr,  \n","    num_train_epochs=num_epochs,  \n","    per_device_train_batch_size=batch_size,  # Batch size for training\n","    per_device_eval_batch_size=batch_size,  # Batch size for evaluation\n","    warmup_steps=500,  # warmup steps for learning rate scheduler\n","    weight_decay=0.01,  \n","    logging_dir='/kaggle/working/logs',  \n","    logging_steps=100,  # Log every 1 steps\n","    #eval_strategy=\"epoch\",  # Evaluate after each epoch\n","    #save_strategy=\"epoch\",  # Save the model after each epoch\n","    eval_strategy=\"steps\",\n","    save_strategy=\"steps\",\n","    eval_steps=500,  # Evaluate every 500 steps\n","    save_steps=1000,  # Save every 500 steps\n","    run_name='bert-finetune-2025_0113_20',\n","    load_best_model_at_end=True,  # Load the best model when finished training\n","    # Enable distributed training\n","    ddp_find_unused_parameters=False,  # Optimizes memory in DDP\n","    fp16=True,\n","    push_to_hub=True,  # Enable pushing to the Hub\n","    hub_model_id=\"anonymousdennis/bert-finetuned-imdb-0113-v2\",  \n","    hub_token=hf_token,\n",")"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-12-31T06:56:40.041443Z","iopub.status.busy":"2024-12-31T06:56:40.041148Z","iopub.status.idle":"2024-12-31T06:56:40.355440Z","shell.execute_reply":"2024-12-31T06:56:40.354487Z","shell.execute_reply.started":"2024-12-31T06:56:40.041421Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2\n","cpu\n","cuda:0\n"]}],"source":["# check device\n","print(torch.cuda.device_count())\n","print(model.device) \n","model.to('cuda')\n","print(model.device) "]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-12-31T06:56:51.738034Z","iopub.status.busy":"2024-12-31T06:56:51.737724Z","iopub.status.idle":"2024-12-31T06:56:55.522593Z","shell.execute_reply":"2024-12-31T06:56:55.521492Z","shell.execute_reply.started":"2024-12-31T06:56:51.738007Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid, fd = os.forkpty()\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]}],"source":["! wandb login -- [your wandb key]"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-12-31T06:56:58.289116Z","iopub.status.busy":"2024-12-31T06:56:58.288763Z","iopub.status.idle":"2024-12-31T07:34:40.272950Z","shell.execute_reply":"2024-12-31T07:34:40.272307Z","shell.execute_reply.started":"2024-12-31T06:56:58.289087Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdelins0803\u001b[0m (\u001b[33mdelins0803-none\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["Tracking run with wandb version 0.19.1"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20241231_065704-8mat9bau</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/delins0803-none/bert-finetune-1230/runs/8mat9bau' target=\"_blank\">sunny-waterfall-2</a></strong> to <a href='https://wandb.ai/delins0803-none/bert-finetune-1230' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/delins0803-none/bert-finetune-1230' target=\"_blank\">https://wandb.ai/delins0803-none/bert-finetune-1230</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/delins0803-none/bert-finetune-1230/runs/8mat9bau' target=\"_blank\">https://wandb.ai/delins0803-none/bert-finetune-1230/runs/8mat9bau</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","You are adding a <class 'transformers.integrations.integration_utils.WandbCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",":DefaultFlowCallback\n","TensorBoardCallback\n","WandbCallback\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='9375' max='9375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [9375/9375 37:17, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.856000</td>\n","      <td>0.402790</td>\n","      <td>{'accuracy': 0.8294}</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.352000</td>\n","      <td>0.356120</td>\n","      <td>{'accuracy': 0.8534}</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.964500</td>\n","      <td>0.340348</td>\n","      <td>{'accuracy': 0.8518}</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.395200</td>\n","      <td>0.342893</td>\n","      <td>{'accuracy': 0.8594}</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.429700</td>\n","      <td>0.327238</td>\n","      <td>{'accuracy': 0.8658}</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.983900</td>\n","      <td>0.453868</td>\n","      <td>{'accuracy': 0.8248}</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.423200</td>\n","      <td>0.383151</td>\n","      <td>{'accuracy': 0.8366}</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.137200</td>\n","      <td>0.365380</td>\n","      <td>{'accuracy': 0.8498}</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>0.371100</td>\n","      <td>0.350100</td>\n","      <td>{'accuracy': 0.8658}</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.090100</td>\n","      <td>0.323258</td>\n","      <td>{'accuracy': 0.8684}</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>0.770700</td>\n","      <td>0.340379</td>\n","      <td>{'accuracy': 0.8682}</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>0.137400</td>\n","      <td>0.324275</td>\n","      <td>{'accuracy': 0.8672}</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>0.888400</td>\n","      <td>0.369931</td>\n","      <td>{'accuracy': 0.8566}</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>0.121500</td>\n","      <td>0.340151</td>\n","      <td>{'accuracy': 0.8646}</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>0.530700</td>\n","      <td>0.343559</td>\n","      <td>{'accuracy': 0.8674}</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>0.036900</td>\n","      <td>0.328106</td>\n","      <td>{'accuracy': 0.8674}</td>\n","    </tr>\n","    <tr>\n","      <td>8500</td>\n","      <td>0.546500</td>\n","      <td>0.327111</td>\n","      <td>{'accuracy': 0.8714}</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>0.312800</td>\n","      <td>0.324318</td>\n","      <td>{'accuracy': 0.8696}</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Trainer is attempting to log a value of \"{'accuracy': 0.8294}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Trainer is attempting to log a value of \"{'accuracy': 0.8534}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Trainer is attempting to log a value of \"{'accuracy': 0.8518}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Trainer is attempting to log a value of \"{'accuracy': 0.8594}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Trainer is attempting to log a value of \"{'accuracy': 0.8658}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Trainer is attempting to log a value of \"{'accuracy': 0.8248}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Trainer is attempting to log a value of \"{'accuracy': 0.8366}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Trainer is attempting to log a value of \"{'accuracy': 0.8498}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Trainer is attempting to log a value of \"{'accuracy': 0.8658}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Trainer is attempting to log a value of \"{'accuracy': 0.8684}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Trainer is attempting to log a value of \"{'accuracy': 0.8682}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Trainer is attempting to log a value of \"{'accuracy': 0.8672}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Trainer is attempting to log a value of \"{'accuracy': 0.8566}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Trainer is attempting to log a value of \"{'accuracy': 0.8646}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Trainer is attempting to log a value of \"{'accuracy': 0.8674}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Trainer is attempting to log a value of \"{'accuracy': 0.8674}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Trainer is attempting to log a value of \"{'accuracy': 0.8714}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","Trainer is attempting to log a value of \"{'accuracy': 0.8696}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▅▅▃▃▂▂▂▂▁▁██▄▄▃▃▂▂▁▁▂▂▁▁▄▄▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▁▅▅▇▇████▆▆▆▆▇▇▆▆▆▆▇▇▆▆▆▆████▆▆████</td></tr><tr><td>eval/samples_per_second</td><td>██▄▄▂▂▁▁▁▁▃▃▃▃▂▂▃▃▃▃▂▂▃▃▃▃▁▁▁▁▃▃▁▁▁▁</td></tr><tr><td>eval/steps_per_second</td><td>██▄▄▂▂▁▁▁▁▃▃▃▃▂▂▃▃▃▃▂▂▃▃▃▃▁▁▁▁▃▃▁▁▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇█</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>train/grad_norm</td><td>▇▂▁█▃▂▅▃▂▃▁▅▂▂▂▂▃▃▃▄▂▂▁▂▂▂▃▄▂▁▂▂▂▃▃▃▄▃▃▄</td></tr><tr><td>train/learning_rate</td><td>█████▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>▂▂▁▃▅▁▁▆▃▂▄█▅▇▄▂▁▁▅▂▃▁▅▄▇▅▂▃▁▄▃▁▁▄█▅▂▃▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.32432</td></tr><tr><td>eval/runtime</td><td>58.1608</td></tr><tr><td>eval/samples_per_second</td><td>85.969</td></tr><tr><td>eval/steps_per_second</td><td>10.746</td></tr><tr><td>total_flos</td><td>9935054899200000.0</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>9375</td></tr><tr><td>train/grad_norm</td><td>15433.56641</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>0.0323</td></tr><tr><td>train_loss</td><td>0.38244</td></tr><tr><td>train_runtime</td><td>2239.5604</td></tr><tr><td>train_samples_per_second</td><td>33.489</td></tr><tr><td>train_steps_per_second</td><td>4.186</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">sunny-waterfall-2</strong> at: <a href='https://wandb.ai/delins0803-none/bert-finetune-1230/runs/8mat9bau' target=\"_blank\">https://wandb.ai/delins0803-none/bert-finetune-1230/runs/8mat9bau</a><br> View project at: <a href='https://wandb.ai/delins0803-none/bert-finetune-1230' target=\"_blank\">https://wandb.ai/delins0803-none/bert-finetune-1230</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20241231_065704-8mat9bau/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["import wandb\n","from transformers.integrations import WandbCallback\n","\n","# Initialize Wandb with project name\n","wandb.init(project=\"bert-finetune-1230\")\n","\n","# Watch the model for parameter and gradient tracking\n","wandb.watch(model)\n","\n","# Create the trainer object\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_data_tokenized,\n","    eval_dataset=val_data_tokenized,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n","    callbacks=[\n","        # Add WandbCallback for automatic logging\n","        WandbCallback() \n","    ]\n",")\n","\n","# Train the model\n","trainer.train()\n","\n","# Finish the run\n","wandb.finish()"]},{"cell_type":"markdown","metadata":{},"source":["# Predict with fine-tuned model"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-12-31T07:48:22.722991Z","iopub.status.busy":"2024-12-31T07:48:22.722683Z","iopub.status.idle":"2024-12-31T07:48:22.895106Z","shell.execute_reply":"2024-12-31T07:48:22.894280Z","shell.execute_reply.started":"2024-12-31T07:48:22.722965Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Trained model predictions:\n","--------------------------\n","If only to avoid making this type of film in the future. - Positive\n","It was amazing. - Positive\n","I want my money back - Positive\n","such a waste of time - Negative\n","Greatest show ever - Positive\n"]}],"source":["# Ensure the model is on the CPU\n","device = torch.device(\"cpu\")\n","model.to(device)\n","\n","text_list = [\"If only to avoid making this type of film in the future.\",\"It was amazing.\", \"I want my money back\",\"such a waste of time\",\"Greatest show ever\"]\n","\n","print(\"Trained model predictions:\")\n","print(\"--------------------------\")\n","for text in text_list:\n","    inputs = tokenizer.encode(text, return_tensors=\"pt\")\n","\n","    logits = trainer.model(inputs).logits\n","    predictions = torch.max(logits,1).indices\n","\n","    print(text + \" - \" + id2label[predictions.tolist()[0]])"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-12-31T07:52:39.951936Z","iopub.status.busy":"2024-12-31T07:52:39.951626Z","iopub.status.idle":"2024-12-31T07:52:47.300989Z","shell.execute_reply":"2024-12-31T07:52:47.300388Z","shell.execute_reply.started":"2024-12-31T07:52:39.951914Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d0020b3f69c248a8b7a949fabbe375be","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/626 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0a1a3f7e0252462682b63778f341942e","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d67c911ba93847e890801bc6dcceaba1","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"accd1d859eb7442f8fbf238a2227602c","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a855a3bfa40145c3a7864d0be7c5f015","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9e57dc8e082943a7b13fbf8bb2a74ce0","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["my_model_name = \"anonymousdennis/bert-finetuned-imdb\"\n","my_model = AutoModelForSequenceClassification.from_pretrained(my_model_name)\n","my_tokenizer = AutoTokenizer.from_pretrained(my_model_name)"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-12-31T07:54:28.998398Z","iopub.status.busy":"2024-12-31T07:54:28.998046Z","iopub.status.idle":"2024-12-31T07:54:29.148355Z","shell.execute_reply":"2024-12-31T07:54:29.147364Z","shell.execute_reply.started":"2024-12-31T07:54:28.998362Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["My kids and I enjoyed watching this! : Positive\n","I wish I could have my 2 hours back : Positive\n","this is GOAT! : Positive\n","undisputablly worst movie ever : Negative\n"]}],"source":["# define list of examples\n","lists = [\"My kids and I enjoyed watching this!\",\"I wish I could have my 2 hours back\",\"this is GOAT!\",\"undisputablly worst movie ever\"]\n","\n","\n","for text in lists:\n","    inputs = my_tokenizer.encode(text, return_tensors=\"pt\")\n","    # compute logits\n","    logits = my_model(inputs).logits\n","    # convert logits to label\n","    predictions = torch.argmax(logits, dim=-1).item()\n","\n","    print(text + \" : \" + id2label[predictions])"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30823,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
