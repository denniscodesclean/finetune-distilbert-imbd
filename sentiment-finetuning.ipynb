{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install  evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T06:54:13.902779Z","iopub.execute_input":"2024-12-31T06:54:13.903068Z","iopub.status.idle":"2024-12-31T06:54:20.158684Z","shell.execute_reply.started":"2024-12-31T06:54:13.903037Z","shell.execute_reply":"2024-12-31T06:54:20.157827Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.5)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.24.7)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.10.5)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nfrom transformers import AutoModelForSeq2SeqLM, AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments,AutoModelForSequenceClassification\nfrom datasets import load_dataset\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport evaluate\nfrom transformers import DataCollatorWithPadding","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-31T06:54:20.193819Z","iopub.execute_input":"2024-12-31T06:54:20.194069Z","iopub.status.idle":"2024-12-31T06:54:46.924457Z","shell.execute_reply.started":"2024-12-31T06:54:20.194050Z","shell.execute_reply":"2024-12-31T06:54:46.923574Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from huggingface_hub import login\n\n# Set the token as an environment variable\nhf_token = \"[my_hf_token]\"\n\nlogin(hf_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T06:54:48.898961Z","iopub.execute_input":"2024-12-31T06:54:48.899568Z","iopub.status.idle":"2024-12-31T06:54:49.049808Z","shell.execute_reply.started":"2024-12-31T06:54:48.899538Z","shell.execute_reply":"2024-12-31T06:54:49.049115Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Import Model\nmodel_name = 'distilbert/distilbert-base-uncased'\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\n\n# Import Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T06:54:50.731797Z","iopub.execute_input":"2024-12-31T06:54:50.732107Z","iopub.status.idle":"2024-12-31T06:54:53.036016Z","shell.execute_reply.started":"2024-12-31T06:54:50.732081Z","shell.execute_reply":"2024-12-31T06:54:53.035238Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36de635eaf3f404485d6a48ac406dd13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45757b0791b54cff9e39271c0552574e"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5ddb6c4ee0847f6b73fdce94b1547de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9db89e12659c4ee8b80fc3c25874265b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7675d8e72f304d338af1d18f0a052086"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Define label maps\nid2label = {0: \"Negative\", 1: \"Positive\"}\nlabel2id = {\"Negative\":0, \"Positive\":1}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T06:54:57.489612Z","iopub.execute_input":"2024-12-31T06:54:57.489922Z","iopub.status.idle":"2024-12-31T06:54:57.493423Z","shell.execute_reply.started":"2024-12-31T06:54:57.489896Z","shell.execute_reply":"2024-12-31T06:54:57.492713Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Data Pre-processing","metadata":{}},{"cell_type":"markdown","source":"## Import Data & Train-test-split","metadata":{}},{"cell_type":"code","source":"# Import Training Dataset\nds = load_dataset('stanfordnlp/imdb')\n\nprint('------Before Splitting------')\nprint(ds['train'].shape)\nprint(ds['test'].shape)\nprint(ds['unsupervised'].shape)\n\ntrain_data = ds['train']\ntest_data_raw = ds['test']\nunsupervised_data = ds['unsupervised']\n\nsplit_ds = test_data_raw.train_test_split(test_size=0.8, stratify_by_column='label', seed=42)\n\n# Extract the validation and test datasets\nval_data = split_ds['train']\ntest_data = split_ds['test']\n\n\nprint('------After Splitting------')\nprint(f'Train Data: {train_data.shape}')\nprint(f'Validation Data: {val_data.shape}')\nprint(f'Test Data: {test_data.shape}')\nprint(f'Unsupervised Data: {unsupervised_data.shape}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T06:54:59.178545Z","iopub.execute_input":"2024-12-31T06:54:59.178966Z","iopub.status.idle":"2024-12-31T06:55:01.997863Z","shell.execute_reply.started":"2024-12-31T06:54:59.178933Z","shell.execute_reply":"2024-12-31T06:55:01.997167Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bf27852c44542a3b2a7a1dd57db9bf0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c155e2f0a18a48bf8bf2b5e5e94ac6e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea7f1a12816d4a93b7a78369ccb0401c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f310222c754d4986bcbc340a2b7ab105"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59fbffe4f2994fd2b5ac701fe1f14e28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31eb7529db18441a8ab09d2e8f2f9aa5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a42a91854ba420e8e9c7eba711db37d"}},"metadata":{}},{"name":"stdout","text":"------Before Splitting------\n(25000, 2)\n(25000, 2)\n(50000, 2)\n------After Splitting------\nTrain Data: (25000, 2)\nValidation Data: (5000, 2)\nTest Data: (20000, 2)\nUnsupervised Data: (50000, 2)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Tokenize","metadata":{}},{"cell_type":"code","source":"def tokenize_function(examples):\n    \"\"\"\n    Tokenizes the input text and moves tensors to the GPU.\n\n    Args:\n        examples: A dictionary containing the input text ('text').\n\n    Returns:\n        A dictionary containing the tokenized inputs (e.g., 'input_ids', 'attention_mask') \n        on the GPU.\n    \"\"\"\n    result = tokenizer(examples['text'], \n                       padding='max_length', \n                       truncation=True, \n                       return_tensors='pt') \n        \n    return result\n\n# Apply the tokenizer to the datasets\ntrain_data_tokenized = train_data.map(tokenize_function, batched=True, load_from_cache_file=False)\nval_data_tokenized = val_data.map(tokenize_function, batched=True, load_from_cache_file=False)\ntest_data_tokenized = test_data.map(tokenize_function, batched=True, load_from_cache_file=False)\n\n# Drop the 'text' column from the datasets\ntrain_data_tokenized = train_data_tokenized.remove_columns(['text'])\nval_data_tokenized = val_data_tokenized.remove_columns(['text'])\ntest_data_tokenized = test_data_tokenized.remove_columns(['text'])\n\n# Initialize the data collator\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\nprint(train_data_tokenized.column_names)\nprint(val_data_tokenized.column_names)\nprint(test_data_tokenized.column_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T06:55:04.625408Z","iopub.execute_input":"2024-12-31T06:55:04.625724Z","iopub.status.idle":"2024-12-31T06:55:33.478965Z","shell.execute_reply.started":"2024-12-31T06:55:04.625702Z","shell.execute_reply":"2024-12-31T06:55:33.478243Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5cb6ee7f53b45f78dabf06617fd1ac1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffde6eede9c34240aad4111d1eb20f58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec32fe1a336548b3997441566076e7d0"}},"metadata":{}},{"name":"stdout","text":"['label', 'input_ids', 'attention_mask']\n['label', 'input_ids', 'attention_mask']\n['label', 'input_ids', 'attention_mask']\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Estimate performance before fine-tuning","metadata":{}},{"cell_type":"code","source":"# define list of examples\ntext_list = [\"If only to avoid making this type of film in the future.\",\"It was amazing.\", \"I want my money back\",\"such a waste of time\",\"Greatest show ever\"]\n\nprint(\"Untrained model predictions: \\n\" )\n\nfor text in text_list:\n    inputs = tokenizer.encode(text, return_tensors=\"pt\")\n    # compute logits\n    logits = model(inputs).logits\n    # convert logits to label\n    predictions = torch.argmax(logits, dim=-1).item()\n\n    print(text + \" : \" + id2label[predictions])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T07:46:59.829432Z","iopub.execute_input":"2024-12-31T07:46:59.829732Z","iopub.status.idle":"2024-12-31T07:46:59.987697Z","shell.execute_reply.started":"2024-12-31T07:46:59.829709Z","shell.execute_reply":"2024-12-31T07:46:59.986806Z"}},"outputs":[{"name":"stdout","text":"Untrained model predictions: \n\nIf only to avoid making this type of film in the future. : Positive\nIt was amazing. : Positive\nI want my money back : Positive\nsuch a waste of time : Negative\nGreatest show ever : Positive\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"# Fine Tuning Starts","metadata":{}},{"cell_type":"code","source":"# Check model architecture\nmodel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T06:40:28.322317Z","iopub.execute_input":"2024-12-31T06:40:28.322712Z","iopub.status.idle":"2024-12-31T06:40:28.329930Z","shell.execute_reply.started":"2024-12-31T06:40:28.322679Z","shell.execute_reply":"2024-12-31T06:40:28.328589Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"DistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0-5): 6 x TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# Freeze base layers parameters, leave Classification Heads unfrozen\nfor param in model.distilbert.parameters():\n    param.requires_grad = False\n\n\n# Unfreeze the classification head (last layer)\nfor param in model.classifier.parameters():\n    param.requires_grad = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T06:55:38.020488Z","iopub.execute_input":"2024-12-31T06:55:38.020779Z","iopub.status.idle":"2024-12-31T06:55:38.025042Z","shell.execute_reply.started":"2024-12-31T06:55:38.020757Z","shell.execute_reply":"2024-12-31T06:55:38.024335Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### Defining computing metric","metadata":{}},{"cell_type":"code","source":"# Load the accuracy metric\naccuracy = evaluate.load(\"accuracy\")\n\ndef compute_metrics(p):\n    # p is an instance of EvalPrediction\n    predictions = np.argmax(p.predictions, axis=1)  # Get the predicted class indices\n    labels = p.label_ids  # True labels\n    \n    # Compute accuracy\n    return {\"accuracy\": accuracy.compute(predictions=predictions, references=labels)}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T06:55:39.617192Z","iopub.execute_input":"2024-12-31T06:55:39.617532Z","iopub.status.idle":"2024-12-31T06:55:40.014037Z","shell.execute_reply.started":"2024-12-31T06:55:39.617504Z","shell.execute_reply":"2024-12-31T06:55:40.013440Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d63868e5e944ad6b50e9a20e130da29"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# hyperparameters\nlr = 1e-3\nbatch_size = 4 # Check GPU Memory to adjust, when batch_size=4, 2 T4 GPUs have ~ 1GB memory out of 15GB.\nnum_epochs = 3\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/results\",  \n    learning_rate=lr,  \n    num_train_epochs=num_epochs,  \n    per_device_train_batch_size=batch_size,  # Batch size for training\n    per_device_eval_batch_size=batch_size,  # Batch size for evaluation\n    #warmup_steps=500,  # Number of warmup steps for learning rate scheduler\n    weight_decay=0.01,  \n    logging_dir='/kaggle/working/logs',  \n    logging_steps=1,  # Log every 1 steps\n    #eval_strategy=\"epoch\",  # Evaluate after each epoch\n    #save_strategy=\"epoch\",  # Save the model after each epoch\n    eval_strategy=\"steps\",\n    save_strategy=\"steps\",\n    eval_steps=500,  # Evaluate every 500 steps\n    save_steps=500,  # Save every 500 steps\n    run_name='bert-finetune-2024_1230_21',\n    load_best_model_at_end=True,  # Load the best model when finished training\n    # Enable distributed training\n    ddp_find_unused_parameters=False,  # Optimizes memory in DDP\n    fp16=True,\n    push_to_hub=True,  # Enable pushing to the Hub\n    hub_model_id=\"anonymousdennis/bert-finetuned-imdb\",  \n    hub_token=hf_token,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T06:56:37.725841Z","iopub.execute_input":"2024-12-31T06:56:37.726138Z","iopub.status.idle":"2024-12-31T06:56:37.878586Z","shell.execute_reply.started":"2024-12-31T06:56:37.726118Z","shell.execute_reply":"2024-12-31T06:56:37.877884Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# check device\nprint(torch.cuda.device_count())\nprint(model.device) \nmodel.to('cuda')\nprint(model.device) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T06:56:40.041148Z","iopub.execute_input":"2024-12-31T06:56:40.041443Z","iopub.status.idle":"2024-12-31T06:56:40.355440Z","shell.execute_reply.started":"2024-12-31T06:56:40.041421Z","shell.execute_reply":"2024-12-31T06:56:40.354487Z"}},"outputs":[{"name":"stdout","text":"2\ncpu\ncuda:0\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"type(train_data_tokenized)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T06:40:57.713840Z","iopub.execute_input":"2024-12-31T06:40:57.714214Z","iopub.status.idle":"2024-12-31T06:40:57.720044Z","shell.execute_reply.started":"2024-12-31T06:40:57.714181Z","shell.execute_reply":"2024-12-31T06:40:57.718919Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"datasets.arrow_dataset.Dataset"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"! wandb login -- [your wandb key]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T06:56:51.737724Z","iopub.execute_input":"2024-12-31T06:56:51.738034Z","iopub.status.idle":"2024-12-31T06:56:55.522593Z","shell.execute_reply.started":"2024-12-31T06:56:51.738007Z","shell.execute_reply":"2024-12-31T06:56:55.521492Z"}},"outputs":[{"name":"stderr","text":"/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import wandb\nfrom transformers.integrations import WandbCallback\n\n# Initialize Wandb with project name\nwandb.init(project=\"bert-finetune-1230\")\n\n# Watch the model for parameter and gradient tracking\nwandb.watch(model)\n\n# Create the trainer object\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_data_tokenized,\n    eval_dataset=val_data_tokenized,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    callbacks=[\n        # Add WandbCallback for automatic logging\n        WandbCallback() \n    ]\n)\n\n# Train the model\ntrainer.train()\n\n# Finish the run\nwandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T06:56:58.288763Z","iopub.execute_input":"2024-12-31T06:56:58.289116Z","iopub.status.idle":"2024-12-31T07:34:40.272950Z","shell.execute_reply.started":"2024-12-31T06:56:58.289087Z","shell.execute_reply":"2024-12-31T07:34:40.272307Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdelins0803\u001b[0m (\u001b[33mdelins0803-none\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241231_065704-8mat9bau</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/delins0803-none/bert-finetune-1230/runs/8mat9bau' target=\"_blank\">sunny-waterfall-2</a></strong> to <a href='https://wandb.ai/delins0803-none/bert-finetune-1230' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/delins0803-none/bert-finetune-1230' target=\"_blank\">https://wandb.ai/delins0803-none/bert-finetune-1230</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/delins0803-none/bert-finetune-1230/runs/8mat9bau' target=\"_blank\">https://wandb.ai/delins0803-none/bert-finetune-1230/runs/8mat9bau</a>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\nYou are adding a <class 'transformers.integrations.integration_utils.WandbCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n:DefaultFlowCallback\nTensorBoardCallback\nWandbCallback\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='9375' max='9375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9375/9375 37:17, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.856000</td>\n      <td>0.402790</td>\n      <td>{'accuracy': 0.8294}</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.352000</td>\n      <td>0.356120</td>\n      <td>{'accuracy': 0.8534}</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.964500</td>\n      <td>0.340348</td>\n      <td>{'accuracy': 0.8518}</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.395200</td>\n      <td>0.342893</td>\n      <td>{'accuracy': 0.8594}</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.429700</td>\n      <td>0.327238</td>\n      <td>{'accuracy': 0.8658}</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.983900</td>\n      <td>0.453868</td>\n      <td>{'accuracy': 0.8248}</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.423200</td>\n      <td>0.383151</td>\n      <td>{'accuracy': 0.8366}</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.137200</td>\n      <td>0.365380</td>\n      <td>{'accuracy': 0.8498}</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.371100</td>\n      <td>0.350100</td>\n      <td>{'accuracy': 0.8658}</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.090100</td>\n      <td>0.323258</td>\n      <td>{'accuracy': 0.8684}</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.770700</td>\n      <td>0.340379</td>\n      <td>{'accuracy': 0.8682}</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.137400</td>\n      <td>0.324275</td>\n      <td>{'accuracy': 0.8672}</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.888400</td>\n      <td>0.369931</td>\n      <td>{'accuracy': 0.8566}</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.121500</td>\n      <td>0.340151</td>\n      <td>{'accuracy': 0.8646}</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.530700</td>\n      <td>0.343559</td>\n      <td>{'accuracy': 0.8674}</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.036900</td>\n      <td>0.328106</td>\n      <td>{'accuracy': 0.8674}</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>0.546500</td>\n      <td>0.327111</td>\n      <td>{'accuracy': 0.8714}</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.312800</td>\n      <td>0.324318</td>\n      <td>{'accuracy': 0.8696}</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Trainer is attempting to log a value of \"{'accuracy': 0.8294}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTrainer is attempting to log a value of \"{'accuracy': 0.8534}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTrainer is attempting to log a value of \"{'accuracy': 0.8518}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTrainer is attempting to log a value of \"{'accuracy': 0.8594}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTrainer is attempting to log a value of \"{'accuracy': 0.8658}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTrainer is attempting to log a value of \"{'accuracy': 0.8248}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTrainer is attempting to log a value of \"{'accuracy': 0.8366}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTrainer is attempting to log a value of \"{'accuracy': 0.8498}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTrainer is attempting to log a value of \"{'accuracy': 0.8658}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTrainer is attempting to log a value of \"{'accuracy': 0.8684}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTrainer is attempting to log a value of \"{'accuracy': 0.8682}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTrainer is attempting to log a value of \"{'accuracy': 0.8672}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTrainer is attempting to log a value of \"{'accuracy': 0.8566}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTrainer is attempting to log a value of \"{'accuracy': 0.8646}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTrainer is attempting to log a value of \"{'accuracy': 0.8674}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTrainer is attempting to log a value of \"{'accuracy': 0.8674}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTrainer is attempting to log a value of \"{'accuracy': 0.8714}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTrainer is attempting to log a value of \"{'accuracy': 0.8696}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▅▅▃▃▂▂▂▂▁▁██▄▄▃▃▂▂▁▁▂▂▁▁▄▄▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▁▅▅▇▇████▆▆▆▆▇▇▆▆▆▆▇▇▆▆▆▆████▆▆████</td></tr><tr><td>eval/samples_per_second</td><td>██▄▄▂▂▁▁▁▁▃▃▃▃▂▂▃▃▃▃▂▂▃▃▃▃▁▁▁▁▃▃▁▁▁▁</td></tr><tr><td>eval/steps_per_second</td><td>██▄▄▂▂▁▁▁▁▃▃▃▃▂▂▃▃▃▃▂▂▃▃▃▃▁▁▁▁▃▃▁▁▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇█</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>train/grad_norm</td><td>▇▂▁█▃▂▅▃▂▃▁▅▂▂▂▂▃▃▃▄▂▂▁▂▂▂▃▄▂▁▂▂▂▃▃▃▄▃▃▄</td></tr><tr><td>train/learning_rate</td><td>█████▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>▂▂▁▃▅▁▁▆▃▂▄█▅▇▄▂▁▁▅▂▃▁▅▄▇▅▂▃▁▄▃▁▁▄█▅▂▃▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.32432</td></tr><tr><td>eval/runtime</td><td>58.1608</td></tr><tr><td>eval/samples_per_second</td><td>85.969</td></tr><tr><td>eval/steps_per_second</td><td>10.746</td></tr><tr><td>total_flos</td><td>9935054899200000.0</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>9375</td></tr><tr><td>train/grad_norm</td><td>15433.56641</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>0.0323</td></tr><tr><td>train_loss</td><td>0.38244</td></tr><tr><td>train_runtime</td><td>2239.5604</td></tr><tr><td>train_samples_per_second</td><td>33.489</td></tr><tr><td>train_steps_per_second</td><td>4.186</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">sunny-waterfall-2</strong> at: <a href='https://wandb.ai/delins0803-none/bert-finetune-1230/runs/8mat9bau' target=\"_blank\">https://wandb.ai/delins0803-none/bert-finetune-1230/runs/8mat9bau</a><br> View project at: <a href='https://wandb.ai/delins0803-none/bert-finetune-1230' target=\"_blank\">https://wandb.ai/delins0803-none/bert-finetune-1230</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20241231_065704-8mat9bau/logs</code>"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"# Predict with fine-tuned model","metadata":{}},{"cell_type":"code","source":"# Ensure the model is on the CPU\ndevice = torch.device(\"cpu\")\nmodel.to(device)\n\ntext_list = [\"If only to avoid making this type of film in the future.\",\"It was amazing.\", \"I want my money back\",\"such a waste of time\",\"Greatest show ever\"]\n\nprint(\"Trained model predictions:\")\nprint(\"--------------------------\")\nfor text in text_list:\n    inputs = tokenizer.encode(text, return_tensors=\"pt\")\n\n    logits = trainer.model(inputs).logits\n    predictions = torch.max(logits,1).indices\n\n    print(text + \" - \" + id2label[predictions.tolist()[0]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T07:48:22.722683Z","iopub.execute_input":"2024-12-31T07:48:22.722991Z","iopub.status.idle":"2024-12-31T07:48:22.895106Z","shell.execute_reply.started":"2024-12-31T07:48:22.722965Z","shell.execute_reply":"2024-12-31T07:48:22.894280Z"}},"outputs":[{"name":"stdout","text":"Trained model predictions:\n--------------------------\nIf only to avoid making this type of film in the future. - Positive\nIt was amazing. - Positive\nI want my money back - Positive\nsuch a waste of time - Negative\nGreatest show ever - Positive\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"my_model_name = \"anonymousdennis/bert-finetuned-imdb\"\nmy_model = AutoModelForSequenceClassification.from_pretrained(my_model_name)\nmy_tokenizer = AutoTokenizer.from_pretrained(my_model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T07:52:39.951626Z","iopub.execute_input":"2024-12-31T07:52:39.951936Z","iopub.status.idle":"2024-12-31T07:52:47.300989Z","shell.execute_reply.started":"2024-12-31T07:52:39.951914Z","shell.execute_reply":"2024-12-31T07:52:47.300388Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/626 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0020b3f69c248a8b7a949fabbe375be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a1a3f7e0252462682b63778f341942e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d67c911ba93847e890801bc6dcceaba1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"accd1d859eb7442f8fbf238a2227602c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a855a3bfa40145c3a7864d0be7c5f015"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e57dc8e082943a7b13fbf8bb2a74ce0"}},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"# define list of examples\nlists = [\"My kids and I enjoyed watching this!\",\"I wish I could have my 2 hours back\",\"this is GOAT!\",\"undisputablly worst movie ever\"]\n\n\nfor text in lists:\n    inputs = my_tokenizer.encode(text, return_tensors=\"pt\")\n    # compute logits\n    logits = my_model(inputs).logits\n    # convert logits to label\n    predictions = torch.argmax(logits, dim=-1).item()\n\n    print(text + \" : \" + id2label[predictions])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T07:54:28.998046Z","iopub.execute_input":"2024-12-31T07:54:28.998398Z","iopub.status.idle":"2024-12-31T07:54:29.148355Z","shell.execute_reply.started":"2024-12-31T07:54:28.998362Z","shell.execute_reply":"2024-12-31T07:54:29.147364Z"}},"outputs":[{"name":"stdout","text":"My kids and I enjoyed watching this! : Positive\nI wish I could have my 2 hours back : Positive\nthis is GOAT! : Positive\nundisputablly worst movie ever : Negative\n","output_type":"stream"}],"execution_count":34}]}